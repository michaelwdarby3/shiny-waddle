For this project, the attached architecture diagram demonstrates a basic implementation of this project, along with several suggestions for each key component. In this file, I'll explain the rationale for each part, starting with the initial HTS library and following the flow throughout the system. I'll additionally be making recommendations primarily within the AWS ecosystem, because even in just the AWS ecosystem there are an abundance of options for every component.

This system assumes you start off with a high-throughput screening (HTS) library, or a dataset of compounds that should be screened. You begin by dividing the HTS library into two parts: the parts to be used for training/testing a model, and the part to be fed into the model later to develop a larger dataset of chemical candidates.

The subset for training and testing the models should be processed into a cloud environment. AWS Glue, Github actions, or even EC2 instances could be used to process the data. Whatever you use, the chemical diagrams should all be converted into a Pytorch Geometric form (or something similar to it that can represent molecules well). This can be uploaded to a data storage solution, depending on the amount of data and how often you want to reaccess the data, as well as how large your budget is. S3 is great for cheap and simple, Redshift has some pretty sophisticated AWS-specific tools for ML tasks (and uses SQL), while Snowflake is heavy-duty but takes longer to learn than most options.

Once your data is stored and you want to start the training process, you can use some messenger system to queue up and send commands to begin your training. The choice here is fairly arbitrary.

The training process itself can be done either by hand on bespoke EC2 instances (with the environment managed by Docker containers on the Elastic Container Registry), or it can be done with a readymade tool, like SageMaker or Databricks. If you're in the experimentation phase, the EC2 instance approach is fine, but it's much easier to handle productionization if you're using a more complete tool. I'd evaluate which tool use by the specific use cases; for example, SageMaker's deployment tooling is really good and easy to work with, while Databricks has exquisite big data tooling to rely on. SageMaker is probably the better tool for this job, but I'd want to check if the graphs needed much analysis before committing to it. Any established tool will have significant support for most model training or monitoring you need, including hyper-parameter tuning, experiment logging and tracking, and drift over time; SageMaker's is above average, so I recommend it again.

You can use any of a hundred tools to monitor or evaluate models as they train and afterwards. Prometheus is a solid choice, and SageMaker specifically has a really robust set of tools for doing the evaluation. Grafana can also do a solid job. It really depends on the specific metrics you need to track down, whether it's loss, convergence time, drift, or overfitting.

Established tooling like SageMaker should support logging plugins; among your options, CloudWatch works great if you're in the AWS ecosystem, Splunk should do the trick, and Grafana can manage it (although less well in my opinion).

Upon a model completing training (the evaluation tools should let you determine which models are good enough and which should be deployed, and you should be testing the models upon completion with a human overseer if possible), you should deploy the model. Two example tools are either Amazon SageMaker, if you've already been using SageMaker and want to be able to deploy with only 2 or 3 lines of code, or AWS Lambda, if you want to keep your deployment model really simple and comparable to a microservice model. The models themselves can be stored using any old storage solution, depending on how they tie into your given deployment solution. The key thing is to maintain model versioning and track updates to models if you do online learning of any sort, as you want to be able to revert easily.

Once you have your models deployed, you should use unlabeled data from your initial HTS library to run through the models and produce inferences. This should provide you with new graph data representative of new potential molecules, which should ideally be translatable into actually producing these molecules in a laboratory. Analyzing these molecules with the same tools that produced the initial HTS library will give you a new dataset like the HTS library; either the molecules produced can be analyzed and tested as they are, or you could feed these molecules into the same process again to produce new models and produce new molecules.

My key concerns that'll lead to specific decisionmaking will be (1) engineer time, (2) money budgeted for cloud ops spending, (3) expected amount of data to be processed and produced, and (4) expected scope creep during and after actual deployment of this system. Using large out of the box tools like SageMaker optimizes for points 1 and 3, but could be hurt pretty badly by point 4. Developing custom tooling or even just stapling together a lot of smaller tools will increase the pain for point 1, but may save money on point 2 and will let you be pretty flexible on points 3 and 4. There's a lot of flexibility to be had here, so the toolset can be selected to your use case pretty easily.